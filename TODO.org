* sisyphus gets in bad state with rabbit when a task fails
* the namespace should be independent of the bucket name
* flow.trigger('sisyphus') gave a json error
* ^C out of flow.listen() should not print a bunch of clutter in ipython
* Sisyphus wrote outputs to GCS after some failed tasks, then retrying the same task names won't start
* a parca task never got picked up by a worker
* WCM output .tgz archives aren't getting stored in GCS; only directory entries are stored
* the Simulation task failed trying to delete the output directory:
    Device or resource busy: '/wcEcoli/out/wf/wildtype_000000/000000/generation_000000/000000/simOut/'

* return the error info (e.g. there's no storage bucket named "robin1") rather than hitting json-decoder-error trying to decode a POST response from the Gaia server
* create intermediate directory entries in object store for nested keys

* support or gracefully-reject relative local paths within the container
* worker nodes need to be robust to task failures
* the queue needs to be robust to task failures; don't rerun them unless that has a reasonable chance of working and there's a max number of retries; the rabbit interaction is failing on error in sisyphus
* auto-create the output bucket?

* store archive with .tgz suffix
* need more error detection & reporting
* probably need worker nodes with more RAM and disk space; maybe configurable
* clear output directories between task runs
* put commands in namespace
* make a Gaia client pip and add it to the wcEcoli requirements, or something
* store timestamp, sisyphus id in logs
* almost always filter logs for a specific job (= user + timestamp); sometimes for a specific task within a job
* make Gaia and Sisyphus logs more informative, less cluttered, and easier to read
** filter by job and optionally by task name or name pattern
** each job should have its own kafka topic(s) for logging, etc.
** perhaps flow.listen() should tune in at the start of the job or from where listen left off
** clearly label the actions in the logs
** clearly label the error messages
** remove internal debugging messages
** label each message for its purpose
** remove the u'text' clutter
** adjustable logging levels
** streamline or strip out JSON data, UUIDs, and such except where it's definitely useful for debugging
** ideally, make a single log entry for a stack traceback
** support stackdriver logging and filtering?

* write a step-by-step how-to document for lab members
** setting the "sisyphus" service account when configuring the GCE instance works, which obviates all the activate-service-account steps

* speed up the workflow
** how come it takes (at least sometimes) many minutes for workers to start picking up tasks?
** tasks run very slowly. do we need VMs with faster CPUs? more RAM? more cores? GPUs? larger disk?
** the log output comes out in batches of lines with many minutes between them

* documentation
** document all the GCE VM setup factors: machine type? boot disk size? OS? Identity and API access? additional access scopes? label e.g. `role=home-base`? startup script? metadata, e.g. configuration for accessing the other servers?

* clarification
** rename 'key' to 'name'? or to '_name' and use sorted key printouts (e.g. via pprint) so the _name shows up first (which is where you need it)

* ensure that running a Command always begins without previous output files even if it reuses an open docker container
* unit tests
* test what happens when things go wrong. does it emit helpful error messages? can it do self-repair?
* use a docker image version tag? how to feed it to the workflow builder?
* auto-launch worker nodes
* ability to post a workflow directly from your desktop?
* tools to simplify and speed up the dev cycle
* implement nightly builds and PR builds
* need DNS names within the cloud rather than hardwired IP addresses
* clean up each workflow job when done
* replace any yaml.load() calls with yaml.safe_load()
* compare cloud shell to a gateway GCE instance
* optimization: reuse a running docker container when the previous task requested the same image
* why do the worker VMs print "*** System restart required ***" when you ssh in?
* remote uploading to Gaia
* remote log monitoring
* remove webserver state viewing
* optimization?: a separate set of nodes for each job
* do we need separate pyenv-virtualenvs to share a gateway machine or to gracefully handle updates?

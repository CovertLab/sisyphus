* mysteries
** (Ryan: validating) sisyphus gets in bad state with rabbit when a task fails
** (Ryan: investigating) Gaia.trigger() doesn't start the workflow unless workers are good and ready
* questions / considerations
** are there rules programmers need to know about tasks deleting and writing into the output directories?
** (Ryan: investigating) is it necessary to have running workers before flow.trigger() will work?
** reevaluate the worker shutdown rules or at least lengthen the worker timeout. (if you start 3 worker nodes for WCM, 2 will take the para and write-json tasks, then the parca task takes so long that the write-json worker gives up.) Maybe workers should stick around while runs (or their run) are running, or maybe we just need to auto-start more of them on demand and make them start up faster.
*** auto-launch and shutdown workers, probably mediated by Gaia. Either accept a worker count from the workflow builder for speed and adjustability or react to the queue length to be more automatic. [Low priority since the WCM builder now does it.]
** (Ryan: validating) worker nodes need to be robust to task failures
** (Ryan: validating) the queue needs to be robust to task failures; don't rerun them unless that has a reasonable chance of working and there's a max number of retries; the rabbit interaction is failing on error in sisyphus
** (Ryan: investigating) rename 'key' to 'name'?
** is the gateway machine obsolete now that the workflow builder can connect to gaia-base and zookeeper-prime?
*** do we need separate pyenv-virtualenvs to share a gateway machine or to gracefully handle updates?
*** compare cloud shell to a gateway GCE instance
** use a docker image version tag? how to feed it to the worker launcher?
* sisyphus
** (Ryan: investigating) have docker output files as the user who starts sisyphus
** service account setup; see https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances : recommends creating a new service account (like "sisyphus") rather than using the Compute Engine default service account; giving it the https://www.googleapis.com/auth/cloud-platform scope; and configuring the instance to run as that service account. Needed for Stackdriver? Presumably we can configure all scopes this way without environment vars.
** create intermediate directory entries in object store for nested keys (it is still needed for gcsfuse; the web console browser doesn't need it)
** support relative local paths within the container (or reject them up front with a clear error message rather than failing obscurely)
** auto-create the output bucket?
* gaia
** standardize gaia API
** put each workflow run in its own namespace such as the GCS prefix "sisyphus:data/jerry/20190709.175452", pass the namespace name in each sisyphus task, and log it in each gaia & sisyphus log entry for filtering
** clean up each workflow run when done
** (Ryan: validating) provide some means to find out what keys the workflow is waiting on (for debugging)
** support task cancellation
** make workers launch quicker. is it quicker to launch a VM from a snapshot or an instance template than an image? easier to resize?
* logs (for now, the logs are the UI)
** support stackdriver logging and filtering: gaia
*** also stackdriver debugger, load trace & profiler, dashboards
*** call the stackdriver API instead of java.util.logging in order to log structured records?
** store a persistant log of high level info plus error messages
** remove kafka-based logging, gaia flow.listen(), and tunneling through to the Kafka cluster
** store timestamp, sisyphus id in logs
** almost always filter logs for a specific run (workflow instance)
** sometimes filter logs for a specific task within a run
** design Gaia and Sisyphus logs to be more informative, less cluttered, easier to filter, and easier to read
** clearly label the action for every log entry
** design the content of each message, e.g.
*** sisyphus-status: {u'status': u'create', u'docker-id': u'8441243d6973', u'id': u'cbb31409-3bc9-4811-94d0-97a0f6bfa3b5', u'docker-config': {u'mounts': {u'/tmp/sisyphus/outputs/data/jerry/20190701.110950/kb': u'/wcEcoli/out/wf/kb'}, u'image': u'gcr.io/allen-discovery-center-mcovert/jerry-wcm-code:latest', u'command': [u'sh', u'-c', u'python -u -m wholecell.fireworks.runTask parca \'{"ribosome_fitting": true, "rnapoly_fitting": true, "cpus": 1, "output_directory": "/wcEcoli/out/wf/kb/"}\'']}}
**** should be more like
*** worker sisyphus-b: python -u -m wholecell.fireworks.runTask parca {"ribosome_fitting": true, "rnapoly_fitting": true, "cpus": 1, "output_directory": "/wcEcoli/out/wf/kb/"}
**** and
*** sisyphus-log: {u'status': u'log', u'line': u'Fitting RNA synthesis probabilities.', u'id': u'cbb31409-3bc9-4811-94d0-97a0f6bfa3b5'}
**** should be more like
*** worker sisyphus-b: Fitting RNA synthesis probabilities.
** filter by run and optionally by task name or name pattern
** each run should have its own kafka topic(s) for logging, etc.
** remove internal debugging messages
** label each message for its purpose
** remove the u'text' clutter
** logging message levels; adjustable log filtering level
** streamline or strip out JSON data, UUIDs, and such except where it's definitely useful for debugging
** adjust Kafka if possible to deliver log entries in smaller batches
* errors
** return the error info (e.g. there's no storage bucket named "robin1") rather than hitting json-decoder-error trying to decode a POST response from the Gaia server
** need more error detection & reporting
** test what happens when things go wrong. does it emit helpful error messages? can it do self-repair?
* optimization
** how come it takes (at least sometimes) many minutes for workers to start picking up tasks?
** tasks run very slowly. do we need VMs with faster CPUs? more RAM? more cores? GPUs? larger disk?
** optimization: reuse a running docker container when the previous task requested the same image
** optimization?: a separate set of nodes for each run
* documentation
** document all the GCE VM setup factors: machine type? boot disk size? OS? Identity and API access? additional access scopes? software installation and configuration? startup script? metadata?
** write a step-by-step how-to document for lab members
*** setting the "sisyphus" service account when configuring the GCE instance works, which obviates all the activate-service-account steps
** document how to create the gaia and sisyphus VM images
** document how to restart and monitor the gaia and sisyphus servers
** document how to make a Compute Engine monitor chart for worker node CPU usage: on GCP dashboard, add chart, Metric instance/cpu/utilization, Filter metric.labels.instance_name = starts_with("sisyphus") and maybe more metrics like instance/disk/read_bytes_count group by project_id aggregate by sum
* features
** unit tests
** web UI: show a graph of your current workflow run's steps, click on a step to see its inputs, outputs, log, and which inputs are available; show the workers and what run/task each one is running
** tools to simplify and speed up the dev cycle
** implement nightly builds and PR builds
** need DNS names within the cloud rather than hardwired IP addresses
** remove webserver state viewing
* DONE
** Sisyphus created empty directories rather than storing archive files for WCM task outputs e.g. sisyphus/data/jerry/20190628.204402/kb/
** Sisyphus created directories for failed tasks e.g. sisyphus/data/jerry/20190628.204402/plotOut/
** pass an array of CLI tokens to Docker so the client doesn't have to do complex shell quoting (jerry put quoting into the WCM workflow as a temporary workaround) (maybe drop the unused && and > features)
** flow.trigger('sisyphus') gave a json error
** Sisyphus wrote outputs to GCS after some failed tasks, so retrying the same task names won't start
** WCM output .tgz archives aren't getting stored in GCS; only directory entries are stored
** clear output directories between task runs
** ensure that running a Command always begins without previous output files even if it reuses an open docker container
** make a Gaia client pip and add it to the wcEcoli requirements, or something
** the sisyphus VM needs more disk space --> now 200GB, 2 CPUs, 7.5 GB RAM
** why do the worker VMs print "*** System restart required ***" when you ssh in? --> the VM image needed rebooting to install updates
** give processes and data keys their own namespace
** the Simulation task failed trying to delete the output directory:
*** Device or resource busy: '/wcEcoli/out/wf/wildtype_000000/000000/generation_000000/000000/simOut/'
** arrange secure access to the Gaia API over the internet
** probably need worker nodes with more RAM and disk space; maybe configurable
** replace any yaml.load() calls with yaml.safe_load()
** remote uploading to Gaia; ability to post a workflow directly from your desktop
** remote log monitoring via flow.listen()
*** give the sisyphus service account permissions to write to logs
** ideally, make a single log entry for a stack traceback
** support stackdriver logging and filtering: sisyphus
** pick an easier way to tunnel to kafka than adding to /etc/hosts (Cloud IAP? ifconfig alias? HOSTALIASES? dynamic port forwarding? VPN?) *OR* obviate it with stackdriver logging
*** [^C out of flow.listen() should not print a bunch of clutter in ipython]
** store archive with .tgz suffix *OR* store the directory of files instead of an archive
** the namespace should be independent of the bucket name
** put commands in namespace
** "gaia-base bash[8924]: WARNING: Illegal reflective access by io.netty.util.internal.ReflectionUtil (file:/home/gaia/.m2/repository/io/netty/netty-all/4.1.11.Final/netty-all-4.1.11.Final.jar) to constructor java.nio.DirectByteBuffer(long,int); Please consider reporting this to the maintainers of io.netty.util.internal.ReflectionUtil; All illegal access operations will be denied in a future release"
** the log output comes out in batches of lines with many minutes between them
** update Gaia.launch(): There's no ../../script/launch-sisyphus.sh in the pip, and it should launch all the servers in one gcloud call like the wcEcoli version does now
** a parca task never got picked up by a worker
** adding workers made everything stop running: with 3 WCM workers, one of them waits and one runs the write-metadata task then times out while the third runs parca. later, I stopped listen(), started 3 more workers, then started listen() again, then it got very stuck. listen() printed nothing. the gaia log only printed Kafka messages about partitions. listen ^C printed the usual stacktrace stuff but wouldn't quit. no ^C response. ^D printed "Do you really want to exit ([y]/n)?" but wouldn't exit. then ^C finally exited.
** log a message when a workflow run stops running and indicate whether all tasks completed successfully
** clearly label the error messages via log/severe! or log/exception!
** perhaps flow.listen() should tune in at the start of the run or from where listen left off
